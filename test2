# -*- coding: utf-8 -*-
"""
Created on Wed Dec 12 16:14:24 2018

@author: l81024549
"""
import pickle
import numpy as np
import tensorflow as tf
def rebuild(imagedata):
    lens = len(imagedata)
    retu = np.zeros((lens,3072),dtype = np.float64)
    for i in range(lens):
        s = imagedata[i]
        s = np.reshape(s,(3,1024))
        s2 = np.zeros((3,1024),dtype = np.float64)
        #print(s)
        tmp = np.zeros((1,1024),dtype = np.float64)
        tmp = (s[0] - np.min(s[0]))/(np.max(s[0])-np.min(s[0]))
        s2[0] = tmp
        tmp = (s[1] - np.min(s[1]))/(np.max(s[1])-np.min(s[1]))
        s2[1] = tmp
        tmp = (s[2] - np.min(s[2]))/(np.max(s[2])-np.min(s[2]))
        s2[2] = tmp
        s2 = s2.flatten()
        retu[i] = s2
    return retu
def unpickle(file):    
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict
def add_layer(inputs,in_size,out_size,activation_function=None):
    w = tf.Variable(tf.random_normal([in_size,out_size]))
    b = tf.Variable(tf.random_normal([1,out_size]))
    f = tf.matmul(inputs,w) + b
    if activation_function is None:
        outputs = f
    else:
        outputs = activation_function(f)
    return outputs
filesize = 3072
dict = {0:[1,0,0,0,0,0,0,0,0,0],1:[0,1,0,0,0,0,0,0,0,0],2:[0,0,1,0,0,0,0,0,0,0]}
dict[3] = [0,0,0,1,0,0,0,0,0,0]
dict[4] = [0,0,0,0,1,0,0,0,0,0]
dict[5] = [0,0,0,0,0,1,0,0,0,0]
dict[6] = [0,0,0,0,0,0,1,0,0,0]
dict[7] = [0,0,0,0,0,0,0,1,0,0]
dict[8] = [0,0,0,0,0,0,0,0,1,0]
dict[9] = [0,0,0,0,0,0,0,0,0,1]
file1 = 'D:\\deepl\\cifar-10-batches-py\\data_batch_1'
file2 = 'D:\\deepl\\cifar-10-batches-py\\data_batch_2'
file3 = 'D:\\deepl\\cifar-10-batches-py\\data_batch_3'
file4 = 'D:\\deepl\\cifar-10-batches-py\\data_batch_4'
file5 = 'D:\\deepl\\cifar-10-batches-py\\data_batch_5'
testfile = 'D:\\deepl\\cifar-10-batches-py\\test_batch'
batch1 = unpickle(file1)
batch1im = batch1.get(b'data')
batch1la = batch1.get(b'labels')

batch2 = unpickle(file2)
batch2im = batch2.get(b'data')
batch2la = batch2.get(b'labels')

batch3 = unpickle(file3)
batch3im = batch3.get(b'data')
batch3la = batch3.get(b'labels')

batch4 = unpickle(file4)
batch4im = batch4.get(b'data')
batch4la = batch4.get(b'labels')

batch5 = unpickle(file5)
batch5im = batch5.get(b'data')
batch5la = batch5.get(b'labels')

images = np.vstack((batch1im,batch2im,batch3im,batch4im,batch5im))
labels = np.hstack((batch1la,batch2la,batch3la,batch4la,batch5la))
labels = np.array(labels)

testbatch = unpickle(testfile)
testimages = testbatch.get(b'data')
testimages = rebuild(testimages)
tmptestlabels = testbatch.get(b'labels')
k = 0
testlabels = np.zeros((10000,10),dtype = np.int32)
for i in tmptestlabels:
    testlabels[k] = dict[i]
    k=k+1

start = 0
size = 100
def get_train_batch():
    global start
    global images
    global labels
    if(start+size>50000):
        index = [i for i in range(len(labels))]
        np.random.shuffle(index)
        images = images[index]
        labels = labels[index]
        start = 0
    end = start+size
    imagesdata = images[start:end]
    tmplabels = labels[start:end]
    labelsdata = np.zeros((100,10),dtype = np.int32)
    k=0
    for i in tmplabels:
        labelsdata[k] = dict[i]
        k=k+1
    start = end
    imagesdata = rebuild(imagesdata)
    return imagesdata,labelsdata

sess=tf.InteractiveSession()
x = tf.placeholder("float", shape=[None, filesize])
y_ = tf.placeholder("float", shape=[None, 10])

length = 600
for layer in range(1):
    if(layer == 0):
        y = add_layer(x,filesize,length,activation_function = tf.nn.sigmoid)
    else:
        y = add_layer(y,length,length,activation_function = tf.nn.sigmoid)
y = add_layer(y,length,10,activation_function = tf.nn.softmax)

cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ *tf.log(y), reduction_indices=[1]))
#regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE, scope=None)
#regulazation = tf.contrib.layers.apply_regularization(regularizer, tf.trainable_variables())
#cross_entropy = cross_entropy + regulazation
train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)
init = tf.initialize_all_variables()
#init = tf.global_variables_initializer()
sess.run(init)


for i in range(20000):
    batch = get_train_batch()
    #print(batch)
    train_step.run(feed_dict={x: batch[0], y_: batch[1]})
print("finish train")
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))  

print(sess.run(y,feed_dict={x:testimages}))
print(sess.run(accuracy, feed_dict={x: testimages, y_: testlabels}))   

sess.close()  
